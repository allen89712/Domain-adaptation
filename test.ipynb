{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network備份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无法加载目标域模型，尝试加载源域模型...\n",
      "测试失败: 源域模型文件也不存在\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.nn.utils.weight_norm as weightNorm\n",
    "from collections import OrderedDict\n",
    "\n",
    "class rbgGenarator(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim):\n",
    "        super(rbgGenarator, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(num_features, hidden_dim, kernel_size=1),  # 1x1 convolution\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1),    # 1x1 convolution\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=1),    # 1x1 convolution\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim, num_features * 2, kernel_size=1),  # 1x1 convolution, output 2x the channels\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)  # 最後加入AdaptiveAvgPool2d，將空間維度壓縮到 1x1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        gamma, beta = torch.split(x, x.size(1) // 2, dim=1)\n",
    "        return gamma, beta\n",
    "\n",
    "\n",
    "class ChannelWiseDynamicNorm(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, eps=1e-5, momentum=0.1):\n",
    "        super(ChannelWiseDynamicNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.rbgen = rbgGenarator(num_features, hidden_dim)\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n",
    "        self.register_buffer(\"running_var\", torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        if self.training:\n",
    "            mean = x.mean(dim=[0, 2, 3], keepdim=True)\n",
    "            var = x.var(dim=[0, 2, 3], keepdim=True, unbiased=False)\n",
    "\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.view(-1)\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.view(-1)\n",
    "\n",
    "            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        else:\n",
    "            mean = self.running_mean.view(1, C, 1, 1)\n",
    "            var = self.running_var.view(1, C, 1, 1)\n",
    "            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "\n",
    "        gamma, beta = self.rbgen(x)\n",
    "\n",
    "        return gamma * x_hat + beta\n",
    "    \n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "class feat_bottleneck(nn.Module):\n",
    "    def __init__(self, feature_dim, bottleneck_dim=256, type=\"ori\"):\n",
    "        super(feat_bottleneck, self).__init__()\n",
    "        self.bn = nn.BatchNorm1d(bottleneck_dim, affine=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.bottleneck = nn.Linear(feature_dim, bottleneck_dim)\n",
    "        self.bottleneck.apply(init_weights)\n",
    "        self.type = type\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bottleneck(x)\n",
    "        if self.type == \"bn\":\n",
    "            x = self.bn(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class feat_classifier(nn.Module):\n",
    "    def __init__(self, class_num, bottleneck_dim=256, type=\"linear\"):\n",
    "        super(feat_classifier, self).__init__()\n",
    "        if type == \"linear\":\n",
    "            self.fc = nn.Linear(bottleneck_dim, class_num)\n",
    "        else:\n",
    "            self.fc = weightNorm(nn.Linear(bottleneck_dim, class_num), name=\"weight\")\n",
    "        self.fc.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class DTNBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DTNBase, self).__init__()\n",
    "        self.conv_params = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.Dropout2d(0.1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.Dropout2d(0.3),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.Dropout2d(0.5),\n",
    "                nn.ReLU()\n",
    "                )   \n",
    "        self.in_features = 256*4*4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_params(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class LeNetBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetBase, self).__init__()\n",
    "        self.conv_params = nn.Sequential(\n",
    "                nn.Conv2d(1, 20, kernel_size=5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(20, 50, kernel_size=5),\n",
    "                nn.Dropout2d(p=0.5),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.ReLU(),\n",
    "                )\n",
    "        self.in_features = 50*4*4\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_params(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
